
# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images/remedy.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[REMEDY: Recipe Merging Dynamics in Large Vision-Language Models
.](https://openreview.net/pdf?id=iX7eHHE5Tx) \\
**Didi Zhu**, Yibing Song, Tao Shen, Ziyu Zhao, Jinluan Yang, Min Zhang, Chao Wu
- First exploration of the LoRA fusion problem in Multimodal Large Language Models
- Proposing a dynamic fusion scheme enhances zero-shot generalization capability of MLLMs.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2024</div><img src='images/model_tailor.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models.](https://arxiv.org/abs/2402.12048) \\
**Didi Zhu**, Zhongyisun Sun, Zexi Li, Tao Shen, Ke Yan, Shouhong Ding, Chao Wu, Kun Kuang

[**Project**](https://github.com/didizhu-zju/Model-Tailor) 

- Pioneered the first comprehensive exploration and revelation of catastrophic forgetting in MLLMs such as InstructBLIP and LLaVa.
- Addressed the issue through an innovative training-free model grafting technique.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">KDD 2024</div><img src='images/npt.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models.](https://arxiv.org/abs/2306.15955) \\
**Didi Zhu**, Zexi Li, Min Zhang, Junkun Yuan, Jiashuo Liu, Kun Kuang, Chao Wu

  - The first exploration of large vision-language models through the lens of neural collapse in deep learning theory.
  - Tackle class imbalance in generalization tasks for large vision-language models by leveraging neural collapse theory.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/uniam.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Universal domain adaptation via compressive attention matching.]([https://openreview.net/forum?id=mvMI3N4AvD](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Universal_Domain_Adaptation_via_Compressive_Attention_Matching_ICCV_2023_paper.pdf)) \\
**Didi Zhu**, Yinchuan Li, Junkun Yuan, Zexi Li, Kun Kuang, Chao Wu

  - Addressed the issue of inconsistent source-target label spaces in Universal Domain Adaptation directly using self-attention in ViT.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM Multimedia 2023</div><img src='images/gflowda.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Generalized Universal Domain Adaptation with Generative Flow Networks.](https://arxiv.org/abs/2305.04466) \\
**Didi Zhu**, Yinchuan Li, Yunfeng Shao, Jianye Hao, Fei Wu, Kun Kuang, Jun Xiao, Chao Wu

- Introduced a comprehensive problem called Generalized Universal Domain Adaptation, achieving a unification of all Domain Adaptation sub-problems involving label heterogeneity.
- Implemented an exploration-aware active learning strategy based on Generative Flow Networks to effectively address GUDA.
</div>
</div>

- `ICML 2025` [ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think](https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gthqIqIAAAAJ&cstart=20&pagesize=80&citation_for_view=gthqIqIAAAAJ:7H_MAutzIkAC), Tao Feng, Wei Li, DiDi Zhu, Hangjie Yuan, Wendi Zheng, Dan Zhang, Jie Tang.
- `ICML 2025` [Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning](https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gthqIqIAAAAJ&citation_for_view=gthqIqIAAAAJ:LgRImbQfgY4C), Wenke Huang, Jian Liang, Zekun Shi, Didi Zhu, Guancheng Wan, He Li, Bo Du, Dacheng Tao, Mang Ye.
- `ICML 2025` [Be Confident: Uncovering Overfitting in MLLM Multi-Task Tuning](), Wenke Huang, Jian Liang, Guancheng Wan, Didi Zhu, He Li, Jiawei Shao, Mang Ye, Bo Du, Dacheng Tao.
- `ICML 2025` [ERICT: Enhancing Robustness by Identifying Concept Tokens in Zero-Shot Vision Language Models](), Xinpeng Dong, Min Zhang, Didi Zhu, Ye Jun Jian, Zhang Keli, Aimin Zhou, Fei Wu, Kun Kuang.
- `ICLR 2025` [Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace](https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gthqIqIAAAAJ&citation_for_view=gthqIqIAAAAJ:nVrZBo8bIpAC),Jinluan Yang, Anke Tang, Didi Zhu, Zhengyu Chen, Li Shen, Fei Wu.
- `ICLR 2025` [Merging loras like playing lego: Pushing the modularity of lora to extremes through rank-wise clustering](https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gthqIqIAAAAJ&citation_for_view=gthqIqIAAAAJ:fbc8zXXH2BUC), Ziyu Zhao, Tao Shen, Didi Zhu, Zexi Li, Jing Su, Xuwu Wang, Kun Kuang, Fei Wu.
- `NeurIPS 2024 Workshop` [Improving Group Connectivity for Generalization of Federated Deep Learning](https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gthqIqIAAAAJ&citation_for_view=gthqIqIAAAAJ:_5tno0g5mFcC), Zexi Li, Jie Lin, Zhiqi Li, Didi Zhu, Rui Ye, Tao Shen, Tao Lin, Chao Wu.
- `KDD 2023` [Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization.](https://arxiv.org/abs/2305.15889), Yunze Tong, Junkun Yuan, Min Zhang, **Didi Zhu**, Keli Zhang, Fei Wu, Kun Kuang.
- `IEEE Transactions on Big Data` [Towards Effective Clustered Federated Learning: A Peer-to-peer Framework with Adaptive Neighbor Matching.]([https://arxiv.org/abs/2305.15889](https://ieeexplore.ieee.org/abstract/document/9954190)), Zexi Li, Jiaxun Lu, Shuang Luo, **Didi Zhu**, Yunfeng Shao, Yinchuan Li, Zhimeng Zhang, Yongheng Wang, Chao Wu.
- `IJCAI 2022 Workshop` [Ensemble federated adversarial training with non-iid data.](https://arxiv.org/abs/2110.14814), Shuang Luo, **Didi Zhu**, Zexi Li, Chao Wu.
