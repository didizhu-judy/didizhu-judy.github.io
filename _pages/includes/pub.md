
# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2024</div><img src='images/model_tailor.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models.]([https://papers.nips.cc/paper/8580-fastspeech-fast-robust-and-controllable-text-to-speech.pdf](https://arxiv.org/abs/2402.12048)) \\
**Didi Zhu**, Zhongyisun Sun, Zexi Li, Tao Shen, Ke Yan, Shouhong Ding, Chao Wu, Kun Kuang

[**Project**](https://github.com/didizhu-zju/Model-Tailor) 

- Pioneered the first comprehensive exploration and revelation of catastrophic forgetting in MLLMs such as InstructBLIP and LLaVa.
- Addressed the issue through an innovative training-free model grafting technique.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">KDD 2024</div><img src='images/npt.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models.](https://arxiv.org/abs/2306.15955) \\
**Didi Zhu**, Zexi Li, Min Zhang, Junkun Yuan, Jiashuo Liu, Kun Kuang, Chao Wu

  - The first exploration of large vision-language models through the lens of neural collapse in deep learning theory.
  - Tackle class imbalance in generalization tasks for large vision-language models by leveraging neural collapse theory.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/uniam.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Universal domain adaptation via compressive attention matching.]([https://openreview.net/forum?id=mvMI3N4AvD](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Universal_Domain_Adaptation_via_Compressive_Attention_Matching_ICCV_2023_paper.pdf)) \\
**Didi Zhu**, Yinchuan Li, Junkun Yuan, Zexi Li, Kun Kuang, Chao Wu

  - Addressed the issue of inconsistent source-target label spaces in Universal Domain Adaptation directly using self-attention in ViT.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM Multimedia 2023</div><img src='images/gflowda.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Generalized Universal Domain Adaptation with Generative Flow Networks.](https://arxiv.org/abs/2305.04466) \\
**Didi Zhu**, Yinchuan Li, Yunfeng Shao, Jianye Hao, Fei Wu, Kun Kuang, Jun Xiao, Chao Wu

- Introduced a comprehensive problem called Generalized Universal Domain Adaptation, achieving a unification of all Domain Adaptation sub-problems involving label heterogeneity.
- Implemented an exploration-aware active learning strategy based on Generative Flow Networks to effectively address GUDA.
</div>
</div>

- `KDD 2023` [Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization.](https://arxiv.org/abs/2305.15889), Yunze Tong, Junkun Yuan, Min Zhang, **Didi Zhu**, Keli Zhang, Fei Wu, Kun Kuang.
- `IEEE Transactions on Big Data` [Towards Effective Clustered Federated Learning: A Peer-to-peer Framework with Adaptive Neighbor Matching.]([https://arxiv.org/abs/2305.15889](https://ieeexplore.ieee.org/abstract/document/9954190)), Zexi Li, Jiaxun Lu, Shuang Luo, **Didi Zhu**, Yunfeng Shao, Yinchuan Li, Zhimeng Zhang, Yongheng Wang, Chao Wu.
- `IJCAI 2022 Workshop` [Ensemble federated adversarial training with non-iid data.](https://arxiv.org/abs/2110.14814), Shuang Luo, **Didi Zhu**, Zexi Li, Chao Wu.
